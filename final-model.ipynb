{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HULAT-UC3M CANTEMIST Participation\n",
    "\n",
    "Team Participants:\n",
    "\n",
    "* Sergio Santamaria Carrasco\n",
    "* Paloma M√°rtinez Fern√°ndez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "1. Set the parent dictory\n",
    "2. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1594735755256,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "MyaKcCaSgVKD"
   },
   "outputs": [],
   "source": [
    "sst_home='/home/sergio/Descargas/TFM/TFM/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgkstAY8h7da"
   },
   "source": [
    "## Text Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8862,
     "status": "ok",
     "timestamp": 1594383821342,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "kdP-lMDch9kB",
    "outputId": "3afe0fab-b422-401a-a3d1-0d2d8dd0805b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANAMNESIS\n",
      "Mujer de 67 a√±os con antecedentes personales de hipotiroidismo en tratamiento con levotiroxina y fumadora activa de 12.5 paquetes/a√±o. Consulta en Urgencias por sensaci√≥n progresiva de ‚Äúacorchamiento y hormigueos‚Äù en ambos miembros superiores e inferiores, as√≠ como una dificultad progresiva para la deambulaci√≥n de 4 meses de evoluci√≥n, asociando asimismo alteraci√≥n de la memoria reciente desde el √∫ltimo mes\n",
      "\n",
      "EXPLORACI√ìN F√çSICA\n",
      "Presentaba una exploraci√≥n cardiopulmonar y abdominal normal. En la exploraci√≥n neurol√≥gica destaca balance motor por grupos musculares conservado; arreflexia rotuliana y aquilea e hiporreflexia bicipital; tetrahipoestesia asim√©trica (t√°ctil, alg√©sica, vibratoria y posicional) en patr√≥n de ‚Äúguante y calcet√≠n altos‚Äù de predominio izquierdo; Romberg positivo y marcha con leve aumento de base de sustentaci√≥n que impresiona ataxia sensitiva.\n",
      "\n",
      "PRUEBAS COMPLEMENTARIAS\n",
      "En las exploraciones complementarias, hemograma, bioqu√≠mica y coagulaci√≥n no presentaban alteraciones. El an√°lisis de virus hepatitis B y C, VIH, las serolog√≠as de enfermedad de Lyme y Treponema pallidum fueron negativas. El estudio lip√≠dico, de\n",
      "vitaminas, proteinograma y anticuerpos antinucleares fueron normales. Se objetiv√≥ positividad para los anticuerpos onconeuronales anfifisina, anti-Hu y anti-SOX-1.\n",
      "\n",
      "Se realiz√≥ una punci√≥n lumbar. El estudio bacteriol√≥gico, de micobacterias, de Herpes virus 1 y 2, Ebstein Barr, Citomegalovirus y Varicela-Zoster fue negativo.\n",
      "\n",
      "El estudio de electroneurograf√≠a-electromiograf√≠a (ENG-EMG) demostr√≥ ausencia de potenciales sensitivos en miembros inferiores y algunos nervios de miembros superiores; siendo las respuestas presentes de amplitud muy reducida, con marcado alargamiento de latencias y disminuci√≥n severa de la velocidad de conducci√≥n. Estudio de conductividad motora y electromiogr√°fico sin hallazgos patol√≥gicos, hallazgos compatibles con polineuropat√≠a sensitiva severa de caracter√≠sticas mixtas.\n",
      "\n",
      "En la tomograf√≠a axial computerizada (TAC) cerebral no se apreciaron alteraciones significativas. En la TAC toraco-abdomino-p√©lvica se observaron una adenopat√≠a parahiliar derecha de 2.5 cm y una adenopat√≠a subcarinal de caracter√≠sticas patol√≥gicas.\n",
      "\n",
      "En el PET-TAC se observaron dep√≥sitos patol√≥gicos que coincid√≠an con los hallazgos del TAC.\n",
      "\n",
      "Se realiz√≥ una ecobroncoscopia lineal con toma de biopsia de adenopat√≠a subcarinal.\n",
      "\n",
      "La resonancia m√°gnetica (RM) cervico-dorso-lumbar mostr√≥ peque√±as hernias discales dorsales y lumbares sin afectaci√≥n del canal medular ni de la cola de caballo.\n",
      "La RM cerebral mostr√≥ en T2 hiperintensidad en ambos l√≥bulos temporales alcanzando hipocampo y corteza siendo sugestivo de encefalitis l√≠mbica.\n",
      "\n",
      "ANATOM√çA PATOL√ìGICA\n",
      "Carcinoma microc√≠tico de pulm√≥n. Inmunohistoqu√≠mica: positivo para TTF1, cromogranina y sinaptofisina, negativo para CK7, CK20 y p40.\n",
      "\n",
      "JUICIO DIAGN√ìSTICO\n",
      "Encefalitis l√≠mbica y polineuropat√≠a sensitiva paraneopl√°sicas secundarias a carcinoma microc√≠tico de pulm√≥n cTxN2 M0 (enfermedad limitada).\n",
      "\n",
      "TRATAMIENTO Y EVOLUCI√ìN\n",
      "Ante el juicio diagn√≥stico, y con la colaboraci√≥n del Servicio de Neurolog√≠a, se inici√≥ tratamiento sintom√°tico secuencial con gammaglobulina (0.4 mg/kg/d√≠a) y corticoides (metilprednisolona 1 g/d√≠a) intravenosos durante 5 d√≠as respectivamente, con escasa mejor√≠a de la cl√≠nica. Debido a las molestias ocasionadas por las parestesias se inici√≥ tratamiento oral con pregabalina a dosis de 50-0-75 mg.\n",
      "\n",
      "Se decidi√≥ comenzar con tratamiento con cisplatino-etop√≥sido m√°s radioterapia concomitante (59.4 Gy en 30 sesiones) a partir del segundo ciclo. Se complet√≥ un total de cinco ciclos observ√°ndose una respuesta parcial radiol√≥gica por criterios RECIST (Response Evaluation Criteria In Solid Tumors).\n",
      "\n",
      "Tres meses despu√©s de la finalizaci√≥n del tratamiento con quimioterapia la paciente refiere empeoramiento progresivo de la cl√≠nica polineurop√°tica, presentando tambi√©n episodios s√∫bitos de fuertes reacciones de miedo, llanto y ansiedad, congruentes con crisis epil√©pticas focales l√≠mbicas, por lo que se decide administrar nueva tanda de gammaglobulinas iv (0,4 mg/kg/d√≠a) e iniciar tratamiento antiepil√©ptico con levetirazetam 500 mg cada 12h y lacosamida 100 mg cada 12h, consiguiendo mejor√≠a sintom√°tica cognitiva y control de las crisis. Posteriormente ante mayor progresi√≥n de la cl√≠nica polineurop√°tica se decide tratamiento de 2¬™ l√≠nea con rituximab iv (1000 mg separados entre s√≠ 14 d√≠as), manteni√©ndose cl√≠nicamente estable.\n",
      "\n",
      "Despu√©s de nueve meses libre de progresi√≥n, se objetiva en el TAC de control un aumento de tejido de partes blandas perihiliar derecho y un n√≥dulo de 7 mm en l√≥bulo inferior derecho no presente en el TAC previo, hipermetab√≥lico en el PET-TC. Por lo tanto, ante una progresi√≥n de enfermedad se decide reiniciar quimioterapia con carboplatinoetop√≥sido al ser una paciente platino sensible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_text = sst_home + 'train-set-to-publish/cantemist-ner/cc_onco1.txt'\n",
    "\n",
    "with open(file_text, 'r') as file:\n",
    "  print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jD5oMJ2ujAZ5"
   },
   "source": [
    "## Annotations Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1593969491633,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "xYXQAbBSjCQQ",
    "outputId": "8f40a2ca-dc7b-4f8e-a38f-ca3101a8feb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1\tMORFOLOGIA_NEOPLASIA 2719 2740\tCarcinoma microc√≠tico\n",
      "T2\tMORFOLOGIA_NEOPLASIA 2950 2971\tcarcinoma microc√≠tico\n",
      "T3\tMORFOLOGIA_NEOPLASIA 2988 2990\tM0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_ann = sst_home + 'train-set-to-publish/cantemist-ner/cc_onco1.ann'\n",
    "\n",
    "with open(file_ann) as file:\n",
    "  print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhdDfygW87Tz"
   },
   "source": [
    "## Requirements\n",
    "\n",
    "1. Tensorflow 1.14.0\n",
    "2. Keras 2.2.4\n",
    "3. Fasttext 0.2.0\n",
    "4. MeaningCloud-python\n",
    "5. Spacy\n",
    "6. Sklearn_crfsuite\n",
    "7. Keras-contrib\n",
    "8. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install MeaningCloud-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28502,
     "status": "ok",
     "timestamp": 1594735785051,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "WHiOfWPE4AlW",
    "outputId": "80d50fd7-9bd7-4623-d5ff-3877b261a323"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59942,
     "status": "ok",
     "timestamp": 1594735823325,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "E7NqI5kGG-NM",
    "outputId": "8eccda49-b6ef-46e9-c703-2e0de109f4bd"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKAteJId5vVO"
   },
   "source": [
    "### Syllabizer\n",
    "\n",
    "Using the rules of Spanish orthography, the syllabizer allows a word to be broken into its component syllables. The necessary code has been extracted from https://github.com/mabodo/sibilizador/blob/master/Silabizator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1977,
     "status": "ok",
     "timestamp": 1594737259810,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "oy2STSpA5uSP"
   },
   "outputs": [],
   "source": [
    "class char():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "class char_line():\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.char_line = [(char, self.char_type(char)) for char in word]\n",
    "        self.type_line = ''.join(chartype for char, chartype in self.char_line)\n",
    "        \n",
    "    def char_type(self, char):\n",
    "        if char in set(['a', '√°', 'e', '√©','o', '√≥', '√≠', '√∫']):\n",
    "            return 'V' #strong vowel\n",
    "        if char in set(['i', 'u', '√º']):\n",
    "            return 'v' #week vowel\n",
    "        if char=='x':\n",
    "            return 'x'\n",
    "        if char=='s':\n",
    "            return 's'\n",
    "        else:\n",
    "            return 'c'\n",
    "            \n",
    "    def find(self, finder):\n",
    "        return self.type_line.find(finder)\n",
    "        \n",
    "    def split(self, pos, where):\n",
    "        return char_line(self.word[0:pos+where]), char_line(self.word[pos+where:])\n",
    "    \n",
    "    def split_by(self, finder, where):\n",
    "        split_point = self.find(finder)\n",
    "        if split_point!=-1:\n",
    "            chl1, chl2 = self.split(split_point, where)\n",
    "            return chl1, chl2\n",
    "        return self, False\n",
    "     \n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.word)\n",
    "\n",
    "class silabizer():\n",
    "    def __init__(self):\n",
    "        self.grammar = []\n",
    "        \n",
    "    def split(self, chars):\n",
    "        rules  = [('VV',1), ('cccc',2), ('xcc',1), ('ccx',2), ('csc',2), ('xc',1), ('cc',1), ('vcc',2), ('Vcc',2), ('sc',1), ('cs',1),('Vc',1), ('vc',1), ('Vs',1), ('vs',1)]\n",
    "        for split_rule, where in rules:\n",
    "            first, second = chars.split_by(split_rule,where)\n",
    "            if second:\n",
    "                if first.type_line in set(['c','s','x','cs']) or second.type_line in set(['c','s','x','cs']):\n",
    "                    #print 'skip1', first.word, second.word, split_rule, chars.type_line\n",
    "                    continue\n",
    "                if first.type_line[-1]=='c' and second.word[0] in set(['l','r']):\n",
    "                    continue\n",
    "                if first.word[-1]=='l' and second.word[-1]=='l':\n",
    "                    continue\n",
    "                if first.word[-1]=='r' and second.word[-1]=='r':\n",
    "                    continue\n",
    "                if first.word[-1]=='c' and second.word[-1]=='h':\n",
    "                    continue\n",
    "                return self.split(first)+self.split(second)\n",
    "        return [chars]\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        return self.split(char_line(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Meaning Cloud Entities\n",
    "\n",
    "MeaningCloud is a Software as a Service product that allows users to embed text analysis and semantic processing into any application or system. Its API, Topic's Extraction, allows extracting the entities present in a text, such as drugs or places. In addition, using a customized dictionary built from the different terms present in the National Cancer Institute's dictionary, we also identify cancer treatments, types of cancer, medical tests, etc.\n",
    "\n",
    "The code above let us extract the entities present in the medical texts and store this information in a dataframe.\n",
    "To be used, is needed has a license key. You can find more information in https://www.meaningcloud.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meaningcloud\n",
    "import os\n",
    "\n",
    "def getMeaningCloudEntities(license_key, sst_home_path):\n",
    "    data = []\n",
    "    for file in [file[:-4] for file in os.listdir(sst_home_path) if file.endswith('.txt') and not ' ' in file]:\n",
    "        file_text = os.path.join(sst_home_path, file + '.txt')\n",
    "\n",
    "        with open(file_text) as f:\n",
    "            text = f.read()\n",
    "\n",
    "        try:\n",
    "            # We are going to make a request to the Topics Extraction API\n",
    "            otherParameters = {'rt': 'y', 'ud': 'Codiesp_CIE10_Diagnostics|cancer_INC'}\n",
    "            topics_response = meaningcloud.TopicsResponse(meaningcloud.TopicsRequest(license_key, txt=text, lang='es',\n",
    "                                                                                     topicType='ec', otherparams = otherParameters).sendReq())\n",
    "\n",
    "            # If there are no errors in the request, we print the output\n",
    "            if topics_response.isSuccessful():\n",
    "                #print(\"\\nThe request to 'Topics Extraction' finished successfully!\\n\")\n",
    "\n",
    "                concepts = topics_response.getConcepts()\n",
    "                if concepts:\n",
    "                    #print(\"\\tConcepts detected (\" + str(len(entities)) + \"):\\n\")\n",
    "                    for concept in concepts:\n",
    "                        form = topics_response.getTopicForm(concept)\n",
    "                        form_type = topics_response.getTypeLastNode(topics_response.getOntoType(concept))\n",
    "                        is_user = topics_response.isUserDefined(concept)\n",
    "                        for appear in concept['variant_list']:\n",
    "                            data.append({'DOC_ID': file, 'FORMA': form, 'TIPO': 'Concept', 'CLASE': form_type, 'INICIO': appear['inip'], 'FINAL': appear['endp'], 'IS_USER_DEFINED': is_user})\n",
    "                        #print(\"\\t\\t\" + topics_response.getTopicForm(concept) + ' --> ' +\n",
    "                              #topics_response.getTypeLastNode(topics_response.getOntoType(concept)) + \"\\n\")\n",
    "\n",
    "                entities = topics_response.getEntities()        \n",
    "                if entities:\n",
    "                    #print(\"\\tEntities detected (\" + str(len(entities)) + \"):\\n\")\n",
    "                    for entity in entities:\n",
    "                        form = topics_response.getTopicForm(entity)\n",
    "                        form_type = topics_response.getTypeLastNode(topics_response.getOntoType(entity))\n",
    "                        is_user = topics_response.isUserDefined(concept)\n",
    "                        for appear in entity['variant_list']:\n",
    "                            data.append({'DOC_ID': file, 'FORMA': form, 'TIPO': 'Entity', 'CLASE': form_type, 'INICIO': appear['inip'], 'FINAL': appear['endp'], 'IS_USER_DEFINED': is_user})\n",
    "                        #print(\"\\t\\t\" + topics_response.getTopicForm(entity) + ' --> ' +\n",
    "                              #topics_response.getTypeLastNode(topics_response.getOntoType(entity)) + \"\\n\")\n",
    "\n",
    "                else:\n",
    "                    print(\"\\tNo entities detected!\\n\")\n",
    "            else:\n",
    "                if topics_response.getResponse() is None:\n",
    "                    print(\"\\nOh no! The request sent did not return a Json\\n\")\n",
    "                else:\n",
    "                    print(\"\\nOh no! There was the following error: \" + topics_response.getStatusMsg() + \"\\n\")\n",
    "\n",
    "        except ValueError:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"\\nException: \" + str(e))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeaningCloudDict(df, doc_id):\n",
    "    data = {}\n",
    "    df = df[df['DOC_ID'] == doc_id]\n",
    "    for instance in df[['FORMA', 'CLASE', 'INICIO', 'FINAL']].iterrows():\n",
    "        row = instance[1]\n",
    "        tokens = nlp(row['FORMA'])\n",
    "        for token in tokens:\n",
    "            ini = row['INICIO'] + token.idx\n",
    "            final = ini + len(token)\n",
    "            data[(ini, final)] = data.get((ini, final), []) + [row['CLASE']]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRAT to BIOES\n",
    "\n",
    "The next code transform the BRAT annotations into BIOES format. The code return a dict with the token offset and the BIOES schema. In this schema,tokens are annotated using the following tags:\n",
    "\n",
    "* B: represents a token that conform the begining of an entity.\n",
    "* I: indicate that the token belongs to an entity.\n",
    "* O: represents that the token does not belong to an entity.\n",
    "* E: marks a token as the end of a given entity.\n",
    "* S: indicates that an entity is comprised of a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# BIOES NOTATION #####################\n",
    "BEGIN = 'B'\n",
    "INSIDE = 'I'\n",
    "OUTSIDE = 'O'\n",
    "END = 'E'\n",
    "SINGLE = 'S'\n",
    "\n",
    "def getDictEntities(file_ann):\n",
    "  entities = {}\n",
    "  with open(file_ann) as anns:\n",
    "    for ann in anns:\n",
    "      ent = ann[:-1].split('\\t')[2]\n",
    "      ent = nlp(ent)\n",
    "      start = int(ann[:-1].split('\\t')[1].split(' ')[1])\n",
    "      end = int(ann[:-1].split('\\t')[1].split(' ')[2])\n",
    "      if (len(ent) == 1):\n",
    "        entities[(start, end)] = SINGLE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "      else:\n",
    "        entities[(start, start + len(ent[0].text))] = BEGIN + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "        entities[(end - len(ent[-1].text)), end] = END + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "        for i in range(len(ent) - 2):\n",
    "          spaces = (ent[i + 1].idx) - (ent[i].idx + len(ent[i].text))\n",
    "          start = start + len(ent[i].text) + spaces\n",
    "          entities[(start, start + len(ent[i + 1].text))] = INSIDE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "\n",
    "  return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zptfUM7JsuQY"
   },
   "source": [
    "# Deep Learning System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for model input\n",
    "\n",
    "We pre-process the text of the clinical cases taking into account different steps.  First, thetexts are split into tokens and sentences using the Spacy, an open-source library that providessupport for texts in several languages, including Spanish.\n",
    "\n",
    "We generate the different attributes considered to be the input of the deep learning stack\n",
    "\n",
    "1. Words: A representation based on pre-trained word embeddings has been used. These vectors have a total of 300 dimensions.\n",
    "\n",
    "2. Part-of-speech: This feature has been considered due to the significant amount ofinformation it offers about the word and its neighbors. It can also help in word sensedisambiguation. The PoS-Tagging model used was the one provided by the Spacy. Anembedding representaton of this feature is learned during training, resulting in a 40-dimensional vector (20 PoS + 20 TAG).\n",
    "\n",
    "3. Characters: We also add character-level embeddings of the words, learned during train-ing and resulting in a 30-dimensional vector. These have proven to be useful for specific-domain tasks and morphologically-rich languages.\n",
    "\n",
    "4. Syllables: Syllable-level embeddings of the words, learned during training and resultingin a 75-dimensional vector is also added. Like character-level embeddings, they help todeal with words outside the vocabulary and contribute to capturing common prefixesand suffixes in the domain and correctly classifying words\n",
    "\n",
    "5. Meaning Cloud Topic Extaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def getElements(sst_home, max_len_seq, concepts_df, classes, getTags = True):\n",
    "  _words = dict()\n",
    "  _doc_tags = {}\n",
    "  _entities = {}\n",
    "  _docs = {}\n",
    "  _docs_offset = {}\n",
    "  mlb = MultiLabelBinarizer(classes = classes)\n",
    "    \n",
    "  for file in [file[:-4] for file in os.listdir(sst_home) if file.endswith('.txt') and not ' ' in file]:\n",
    "    concepts_dict = getMeaningCloudDict(concepts_df, file)\n",
    "    \n",
    "    file_text = os.path.join(sst_home, file + '.txt')\n",
    "    if getTags:\n",
    "      file_ann = os.path.join(sst_home, file + '.ann')\n",
    "      _entities = getDictEntities(file_ann)\n",
    "    with open(file_text) as f:\n",
    "      #re_date = re.compile(r\"\\d{1,2}/\\d{1,2}/\\d{4}\")\n",
    "      _sents = []\n",
    "      _sents_lemma = []\n",
    "      _tags = []\n",
    "      _sents_pos = []\n",
    "      _sents_gtags = []\n",
    "      _sents_offset = []\n",
    "      _sents_concepts = []\n",
    "      text = f.read()\n",
    "      sections = text.split('\\n\\n')\n",
    "      offset = 0\n",
    "      for section in sections:\n",
    "        sents = section.split('\\n')\n",
    "        for sent in sents:\n",
    "          sent_nlp = nlp(sent)\n",
    "          stop_sent = [token for token in sent_nlp if not token.is_stop]\n",
    "          for i in range(0, len(stop_sent), max_len_seq):\n",
    "            _sent = []\n",
    "            _sent_tags = []\n",
    "            _sent_pos = []\n",
    "            _sent_gtags = []\n",
    "            _sent_offset = []\n",
    "            _sent_concepts = []\n",
    "            _sent_lemma = []\n",
    "            for token in stop_sent[i:i+max_len_seq]:\n",
    "              _sent.append(token.text)\n",
    "              _sent_lemma.append(token.lemma_)\n",
    "              _sent_offset.append((token.text, token.idx + offset, token.idx + offset + len(token.text)))\n",
    "              _entity = _entities.get((token.idx + offset, token.idx + offset + len(token.text)), OUTSIDE)\n",
    "              _sent_tags.append(_entity)\n",
    "              _words[token.text] = _words.get(token.text, 0) + 1\n",
    "              _sent_pos.append(token.pos_)\n",
    "              _sent_gtags.append(token.tag_.split('_')[0])\n",
    "              _sent_concepts.append(concepts_dict.get((token.idx + offset, token.idx + offset + len(token.text)), []))\n",
    "\n",
    "            _sents.append(_sent)\n",
    "            _sents_lemma.append(_sent_lemma)\n",
    "            _tags.append(_sent_tags)\n",
    "            _sents_pos.append(_sent_pos)\n",
    "            _sents_gtags.append(_sent_gtags)\n",
    "            _sents_offset.append(_sent_offset)\n",
    "            _mlb_concepts = mlb.fit_transform(_sent_concepts)\n",
    "            _sents_concepts.append(_mlb_concepts)\n",
    "            \n",
    "\n",
    "          offset = offset + len(sent) + 1\n",
    "        offset = offset + 1\n",
    "\n",
    "      _docs[file] = (_sents, _sents_pos, _sents_gtags, _sents_concepts, _sents_lemma)\n",
    "      _doc_tags[file] = _tags\n",
    "      _docs_offset[file] = _sents_offset\n",
    "\n",
    "  #_words = list(_words)\n",
    "\n",
    "  return _words, _docs, _doc_tags, _docs_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1594736945638,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "DsXymUb10gNR"
   },
   "outputs": [],
   "source": [
    "def get2idx(words, chars, pos, gramm_tags, sylls):\n",
    "  TM = '_MORFOLOGIA_NEOPLASIA'\n",
    "  word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "  word2idx[\"#ENDPAD\"] = 0\n",
    "  word2idx[\"UNK\"] = 1\n",
    "\n",
    "  char2idx = {char:i + 2 for i,char in enumerate(chars)}\n",
    "  char2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  char2idx[\"UNK\"] = 1\n",
    "\n",
    "  pos2idx = {pos:i + 2 for i,pos in enumerate(pos)}\n",
    "  pos2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  pos2idx[\"UNK\"] = 1\n",
    "\n",
    "  grammtags2idx = {tag:i + 2 for i,tag in enumerate(gramm_tags)}\n",
    "  grammtags2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  grammtags2idx[\"UNK\"] = 1\n",
    "\n",
    "  sylls2idx = {syll:i + 2 for i,syll in enumerate(sylls)}\n",
    "  sylls2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  sylls2idx[\"UNK\"] = 1\n",
    "\n",
    "  tag2idx = {tag:i + 1 for i,tag in enumerate([BEGIN + TM, INSIDE + TM, END + TM, OUTSIDE, SINGLE + TM])}\n",
    "  tag2idx[\"#ENDPAD\"] = 0 \n",
    "\n",
    "  return word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1594736950876,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "naBOzTWcIHXW"
   },
   "outputs": [],
   "source": [
    "def getCharacterInput(sentences, max_seq_len, max_chars_len, char2idx):\n",
    "  X_char = []\n",
    "  for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    # max_len = 75\n",
    "    for i in range(max_seq_len):\n",
    "      word_seq = []\n",
    "      # char sequence for words\n",
    "      for j in range(max_chars_len):\n",
    "        try:\n",
    "          # chars of specific sentence of i\n",
    "          word_seq.append(char2idx.get(sentence[i][j], 1)) \n",
    "        except:  # if char-sequence is out of range , pad it with \"PAD\" tag\n",
    "          word_seq.append(char2idx.get(\"#ENDPAD\"))\n",
    "\n",
    "      sent_seq.append(word_seq)\n",
    "    # append sentence sequences as character-by-character to X_char for Model input\n",
    "    X_char.append(np.array(sent_seq))\n",
    "\n",
    "  return np.array(X_char)\n",
    "\n",
    "def getSyllsInput(sentences, max_seq_len, max_sylls_len, sylls2idx):\n",
    "  X_syll = []\n",
    "  for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    # max_len = 75\n",
    "    for i in range(max_seq_len):\n",
    "      word_seq = []\n",
    "      syllables = []\n",
    "      try:\n",
    "        syllables = silabizer(sentence[i])\n",
    "      except:\n",
    "        pass\n",
    "      for j in range(max_sylls_len):\n",
    "        try:\n",
    "          # chars of specific sentence of i\n",
    "          word_seq.append(sylls2idx.get(str(syllables[j]).lower(), 1)) \n",
    "        except:  # if char-sequence is out of range , pad it with \"PAD\" tag\n",
    "          word_seq.append(sylls2idx.get(\"#ENDPAD\"))\n",
    "\n",
    "      sent_seq.append(word_seq)\n",
    "    # append sentence sequences as character-by-character to X_char for Model input\n",
    "    X_syll.append(np.array(sent_seq))\n",
    "\n",
    "  return np.array(X_syll)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def getWordInput(sentences, max_seq_len, word2idx):\n",
    "  X = [[word2idx.get(w,1) for w in s[:max_seq_len]] for s in sentences]\n",
    "  X = pad_sequences(maxlen = max_seq_len, sequences = X, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X)\n",
    "\n",
    "def getPosInput(sentences, max_seq_len, pos2idx):\n",
    "  X_pos = [[pos2idx.get(w,1) for w in s] for s in sentences]\n",
    "  X_pos = pad_sequences(maxlen = max_seq_len, sequences = X_pos, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X_pos)\n",
    "\n",
    "def getGTagInput(sentences, max_seq_len, grammtags2idx):\n",
    "  X_gramm_tags = [[grammtags2idx.get(w,1) for w in s] for s in sentences]\n",
    "  X_gramm_tags = pad_sequences(maxlen = max_seq_len, sequences = X_gramm_tags, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X_gramm_tags)\n",
    "\n",
    "def getConceptsInput(sentences, max_seq_len, classes):\n",
    "  X = pad_sequences(maxlen = max_seq_len, sequences = sentences, truncating= 'post', padding ='post', value=np.zeros(len(classes)) )\n",
    "  return np.array(X)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def getY(tags, max_seq_len, tag2idx):\n",
    "  y = [[tag2idx[t] for t in sent_tags] for sent_tags in tags]\n",
    "  y = pad_sequences(maxlen = max_seq_len, sequences = y, padding = \"post\", truncating = \"post\", value = tag2idx[\"#ENDPAD\"])\n",
    "  \n",
    "  y = [to_categorical(i, num_classes = 6) for i in y]\n",
    "  return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings from Spanish Medical Corpora\n",
    "\n",
    "The word embeddings used can be found in https://www.aclweb.org/anthology/W19-1916/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 70831,
     "status": "ok",
     "timestamp": 1594737026503,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "9qZMrEmvenuz",
    "outputId": "327c6b92-4e95-4fab-c68f-ac5e1af6623f"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "sst_home_embeddings = sst_home + 'fast-text-model/'\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'cantemist-resource.bin')\n",
    "\n",
    "def getEmbeddingMatrix(words2idx, emb_dim, model):\n",
    "  embedding_matrix = np.zeros((len(words2idx), emb_dim))\n",
    "  for word, i in words2idx.items():\n",
    "    embedding_matrix[i] = model[word]\n",
    "\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cause we are using pre-trained embeddings, we use as our vocabulary the words contained in both the training, development and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_total_words = set()\n",
    "sst_home_test = sst_home + 'train-set-to-publish/cantemist-ner/'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_test) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_test, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                _total_words.add(token.text)\n",
    "             \n",
    "sst_home_test = sst_home + 'dev-set-to-publish/'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_test) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_test, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                _total_words.add(token.text)\n",
    "                \n",
    "sst_home_test = sst_home + 'test-background-set-to-publish/'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_test) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_test, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                _total_words.add(token.text)\n",
    "                \n",
    "_words = list(_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training Data\n",
    "\n",
    "The next code generate the training data. We set the next hyperparameters:\n",
    "\n",
    "1. Max sequence length in words: 50\n",
    "2. Max sequence length in characters: 30\n",
    "3. Max sequence length in syllables: 10\n",
    "4. The entities to take into account during the extraction of Meaninging Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 967816,
     "status": "ok",
     "timestamp": 1594738236258,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "73oDT5CIR4ri"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst_home_train = sst_home + 'final-training/'\n",
    "\n",
    "MAX_CHARS_LEN = 30\n",
    "MAX_SEQ_LEN = 50\n",
    "MAX_SYLLS_LEN = 10\n",
    "\n",
    "silabizer = silabizer()\n",
    "classes = ['Top', 'Disease', 'DIAGN√ìSTICOS', 'MedicalTest', 'BodyPart',\n",
    "       'Cancer', 'CancerDetection', 'CancerTreatment', 'Drug', 'PhysicalExtentUnit', 'WeightUnit', 'SpaceUnit',\n",
    "          'VolumeUnit','BenignTumor', 'Unit']\n",
    "\n",
    "df_concepts = pd.read_csv(sst_home + 'update_info.csv')\n",
    "df_concepts = df_concepts[df_concepts['CLASE'].isin(classes)]\n",
    "df_concepts.drop_duplicates()\n",
    "\n",
    "_words_in_tr, _docs, _docs_tags, _ = getElements(sst_home_train, MAX_SEQ_LEN, df_concepts, classes)\n",
    "_words_in_tr = list(_words_in_tr.keys())\n",
    "_chars = list(set(''.join(_words_in_tr)))\n",
    "_sylls = list(set(str(syll).lower() for word in _words_in_tr for syll in silabizer(word)))\n",
    "\n",
    "\n",
    "_sents_train = [sent for sents in _docs.values() for sent in sents[0]]\n",
    "_sents_pos = [sent for sents in _docs.values() for sent in sents[1]]\n",
    "_sents_gtags = [sent for sents in _docs.values() for sent in sents[2]]\n",
    "_sents_concepts = [sent for sents in _docs.values() for sent in sents[3]]\n",
    "_sents_lemma_train = [sent for sents in _docs.values() for sent in sents[4]]\n",
    "\n",
    "_pos = set(pos for sent in _sents_pos for pos in sent)\n",
    "_gramm_tags = set(gtag for sent in _sents_gtags for gtag in sent)\n",
    "\n",
    "_tags = [tag for tags in _docs_tags.values() for tag in tags]\n",
    "\n",
    "word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx = get2idx(_words, _chars, _pos, _gramm_tags, _sylls)\n",
    "\n",
    "X_char_train = getCharacterInput(_sents_train, MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "X_words_train = getWordInput(_sents_train, MAX_SEQ_LEN, word2idx)\n",
    "X_pos_train = getPosInput(_sents_pos, MAX_SEQ_LEN, pos2idx)\n",
    "X_tag_train = getGTagInput(_sents_gtags, MAX_SEQ_LEN, grammtags2idx)\n",
    "X_syll_train = getSyllsInput(_sents_train, MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "X_concepts_train = getConceptsInput(_sents_concepts, MAX_SEQ_LEN, classes)\n",
    "\n",
    "y_train = getY(_tags, MAX_SEQ_LEN, tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Validation Data\n",
    "\n",
    "The next code generate the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1442312,
     "status": "ok",
     "timestamp": 1594738715620,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "0i5wkvcXdjmn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst_home_test = sst_home + 'dev-set1-to-publish/cantemist-ner/'\n",
    "\n",
    "_, _docs, _docs_tags, offset = getElements(sst_home_test, MAX_SEQ_LEN, df_concepts, classes)\n",
    "\n",
    "_sents_val = [sent for sents in _docs.values() for sent in sents[0]]\n",
    "_sents_pos = [sent for sents in _docs.values() for sent in sents[1]]\n",
    "_sents_gtags = [sent for sents in _docs.values() for sent in sents[2]]\n",
    "_sents_concepts = [sent for sents in _docs.values() for sent in sents[3]]\n",
    "_sents_lemma_test = [sent for sents in _docs.values() for sent in sents[4]]\n",
    "\n",
    "_tags = [tag for tags in _docs_tags.values() for tag in tags]\n",
    "\n",
    "\n",
    "X_char_test = getCharacterInput(_sents_val, MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "X_words_test = getWordInput(_sents_val, MAX_SEQ_LEN, word2idx)\n",
    "X_pos_test = getPosInput(_sents_pos, MAX_SEQ_LEN, pos2idx)\n",
    "X_tag_test = getGTagInput(_sents_gtags, MAX_SEQ_LEN, grammtags2idx)\n",
    "X_syll_test = getSyllsInput(_sents_val, MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "X_concepts_test = getConceptsInput(_sents_concepts, MAX_SEQ_LEN, classes)\n",
    "\n",
    "y_test = getY(_tags, MAX_SEQ_LEN, tag2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix \n",
    "\n",
    "The next code generate the embedding matrix to be used in the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1367,
     "status": "ok",
     "timestamp": 1594738739088,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "Zr7zrx-XpHLM"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = getEmbeddingMatrix(word2idx, ft.get_dimension(), ft)\n",
    "del ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data\n",
    "\n",
    "We save the generated data so that it is not necessary to generate it again next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3736,
     "status": "ok",
     "timestamp": 1594738743645,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "7NsCvbe8Yuo1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_data = [X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_concepts_train, y_train]\n",
    "with open(sst_home + 'data_final/train.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "test_data = [X_char_test, X_words_test, X_pos_test, X_tag_test, X_syll_test, X_concepts_test, y_test]\n",
    "with open(sst_home + 'data_final/test.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(sst_home + 'data_final/we.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "two2idx = [word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx]\n",
    "with open(sst_home + 'data_final/2idx.pickle', 'wb') as handle:\n",
    "    pickle.dump(two2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "In case the data is previously saved, we load these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2102,
     "status": "ok",
     "timestamp": 1594735844585,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "-cilCnz9Zohv"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_train = []\n",
    "with open(sst_home + 'data_final/train.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "\n",
    "X_test = []\n",
    "with open(sst_home + 'data_final/test.pickle', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "\n",
    "embedding_matrixes = []\n",
    "with open(sst_home + 'data_final/we.pickle', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "two2idx = []\n",
    "with open(sst_home + 'data_final/2idx.pickle', 'rb') as handle:\n",
    "    two2idx = pickle.load(handle)\n",
    "\n",
    "X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_concepts_train, y_train = X_train\n",
    "X_char_test, X_words_test, X_pos_test, X_tag_test, X_syll_test, X_concepts_test, y_test = X_test\n",
    "word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx = two2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Architecture\n",
    "\n",
    "The model implemented, as shown in the figure, works on two levels of maximum sequence word length.\n",
    "\n",
    "First, a bidirectional LSTM layer receives the input features, the sequence of characters and syllables being previously processed by a convolutional and global max pooling block, for a maximum sequence length of 10. \n",
    "\n",
    "The reshaped output of this layer is concatenated with the input features, but this time for a maximum sequence length size of 50, and serves as the input for a new BiLSTM. This layer connects directly to a fully connected dense layer with ùë°ùëéùëõ‚Ñé activation function.\n",
    "\n",
    "The last layer (CRF optimization layer) consists of a conditional random fields layer. This layer receives as input the sequence of probabilities of the previous layer in order to improve predictions. This is due to the ability of the layer to take into account the dependencies between the different labels. The output of this layer provides the most probable sequence of labels.\n",
    "\n",
    "![Architecture of the proposed model for named entity recognition.](imgs/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5840,
     "status": "ok",
     "timestamp": 1594738835549,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "nnl7WR0TX7Yn",
    "outputId": "4c7d7661-a8e6-443a-a62e-c7940f9a2901"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Concatenate, Input, SpatialDropout1D\n",
    "from keras.layers import Conv1D, MaxPooling1D,Flatten,GlobalMaxPooling1D, Reshape, RepeatVector\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "CHAR_EMBEDDINGS_SIZE = 30    # Characters Embeddings Size\n",
    "SYLL_EMBEDDINGS_SIZE = 75    # Syllable Embeddings Size\n",
    "WORD_EMBEDDINGS_SIZE = 300   # Word Embeddings Size\n",
    "POS_EMBEDDING_SIZE = 20      # PoS Embedding Size\n",
    "GTAGS_EMBEDDING_SIZE = 20    # Tag Embedding Size\n",
    "\n",
    "N_CLASSES = len(classes)     # Number of different entities in Meaning Cloud\n",
    "\n",
    "MAX_CHARS_LEN = 30           # Max sequence char length\n",
    "MAX_SEQ_LEN = 50             # Max sequence word length Level II\n",
    "MAX_SYLLS_LEN = 10           # Max sequence sylls length\n",
    "\n",
    "LOCAL_CONT_LEN = 10          # Max sequence word length Level I   \n",
    "LOCAL_CONT_SPLIT = MAX_SEQ_LEN // LOCAL_CONT_LEN\n",
    "\n",
    "CONV_FILTERS = 50            # Convolutional Filters in Character and Syllable Convolutional Layer\n",
    "LSTM_UNITS = 300             # LSTM Units in both LSTM layers\n",
    "DENSE_UNITS = 200            # Number of units in Dense layer  \n",
    "\n",
    "####################### INPUT DATA ################################\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), name = 'word_input')\n",
    "pos_input = Input(shape=(MAX_SEQ_LEN,), name = 'pos_input')\n",
    "gtag_input = Input(shape=(MAX_SEQ_LEN,), name = 'tag_input')\n",
    "concepts_input = Input(shape=(MAX_SEQ_LEN, N_CLASSES), name = 'meaning_cloud_input')\n",
    "char_input = Input(shape=(MAX_SEQ_LEN, MAX_CHARS_LEN), name = 'char_input')\n",
    "sylls_input = Input(shape=(MAX_SEQ_LEN, MAX_SYLLS_LEN), name = 'sylls_input')\n",
    "\n",
    "\n",
    "local_context_input = Reshape((LOCAL_CONT_SPLIT, LOCAL_CONT_LEN,), input_shape=(MAX_SEQ_LEN,))(word_input)\n",
    "\n",
    "###################### CHARACTER SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "char_embedding = TimeDistributed(Embedding(input_dim = len(char2idx), output_dim = CHAR_EMBEDDINGS_SIZE, input_length=MAX_CHARS_LEN, name = 'char_embeddings', trainable = True))(char_input)\n",
    "conv_1d = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding=\"valid\", activation=\"relu\", name=\"Conv1D_char\"))(char_embedding)\n",
    "conv_1d = TimeDistributed(Dropout(0.4))(conv_1d)\n",
    "maxpool1d = TimeDistributed(GlobalMaxPooling1D(), name = 'max_pooling')(conv_1d)\n",
    "char_enc = TimeDistributed(Flatten(), name = 'char_enc')(maxpool1d)\n",
    "\n",
    "###################### SYLLABLE SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "syll_embedding = TimeDistributed(Embedding(input_dim = len(sylls2idx), output_dim = SYLL_EMBEDDINGS_SIZE, input_length=MAX_SYLLS_LEN, name = 'sylls_embeddings', trainable = True))(sylls_input)\n",
    "conv_1d_syll = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding=\"valid\", activation=\"relu\", name=\"Conv1D_syll\"))(syll_embedding)\n",
    "conv_1d_syll = TimeDistributed(Dropout(0.4))(conv_1d_syll)\n",
    "maxpool1d_syll = TimeDistributed(GlobalMaxPooling1D())(conv_1d_syll)\n",
    "syll_enc = TimeDistributed(Flatten())(maxpool1d_syll)\n",
    "\n",
    "##################### PoS + TAG EMBEDDINGS ################################################################\n",
    "pos_embedding = Embedding(input_dim = len(pos2idx), output_dim = POS_EMBEDDING_SIZE, input_length = MAX_SEQ_LEN, name = 'pos_embeddings', trainable = True)(pos_input)\n",
    "gtags_embedding = Embedding(input_dim = len(grammtags2idx), output_dim = GTAGS_EMBEDDING_SIZE, input_length = MAX_SEQ_LEN, name = 'gtags_embeddings', trainable = True)(gtag_input)\n",
    "\n",
    "##################### WORD EMBEDDING LAYER ################################################################\n",
    "embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[embedding_matrix], trainable=True, name = 'word_embeddings')\n",
    "word_embedding = embedding_layer(word_input)\n",
    "\n",
    "\n",
    "#################### RESHAPE TO SET MAX SEQUENCE WORD LENGTH TO 10 ########################################\n",
    "local_word_embedding = TimeDistributed(embedding_layer, name = 'local_context_embedding')(local_context_input)\n",
    "local_char = Reshape((LOCAL_CONT_SPLIT, LOCAL_CONT_LEN, CONV_FILTERS), input_shape=(MAX_SEQ_LEN, CONV_FILTERS))(char_enc)\n",
    "local_syll = Reshape((LOCAL_CONT_SPLIT, LOCAL_CONT_LEN, CONV_FILTERS), input_shape=(MAX_SEQ_LEN, CONV_FILTERS))(syll_enc)\n",
    "local_pos = Reshape((LOCAL_CONT_SPLIT, LOCAL_CONT_LEN, POS_EMBEDDING_SIZE), input_shape=(MAX_SEQ_LEN, 100))(pos_embedding)\n",
    "local_gtag = Reshape((LOCAL_CONT_SPLIT, LOCAL_CONT_LEN, GTAGS_EMBEDDING_SIZE), input_shape=(MAX_SEQ_LEN, 100))(gtags_embedding)\n",
    "local_concepts = Reshape((LOCAL_CONT_SPLIT, LOCAL_CONT_LEN, N_CLASSES), input_shape=(MAX_SEQ_LEN, 100))(concepts_input)\n",
    "\n",
    "#################### BiLSTM MAX SEQUENCE WORD LENGTH 10 ###################################################\n",
    "local_info = Concatenate(axis = -1)([local_word_embedding, local_pos, local_gtag, local_char, local_syll, local_concepts])\n",
    "local_cont = TimeDistributed(Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True, recurrent_dropout=0.4)))(local_info)\n",
    "local_cont = TimeDistributed(Dropout(0.4))(local_cont)\n",
    "\n",
    "\n",
    "################### RESHAPE TO SET MAX SEQUENCE WORD LENGTH TO 50 ##########################################\n",
    "local_cont = Reshape((MAX_SEQ_LEN, 2*LSTM_UNITS), input_shape=(LOCAL_CONT_SPLIT, LOCAL_CONT_LEN, 2*LSTM_UNITS))(local_cont)\n",
    "\n",
    "################### CONCATENATE BiLSTM OUTPUT AND INPUT FEATURES ###########################################\n",
    "x = Concatenate(axis = -1)([word_embedding, pos_embedding, gtags_embedding, char_enc, syll_enc, concepts_input, local_cont])\n",
    "\n",
    "################## BiLSTM MAX SEQUENCE WORD LENGTH 50 ######################################################\n",
    "x = Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True,recurrent_dropout=0.4))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "################## DENSE LAYER ############################################################################\n",
    "x = Dense(DENSE_UNITS, activation='tanh')(x)\n",
    "\n",
    "################## CRF LAYER ####################################################################################\n",
    "crf = CRF(len(tag2idx), sparse_target = False)\n",
    "loss = crf.loss_function\n",
    "y_output = crf(x)\n",
    "\n",
    "model = Model(inputs = [char_input, word_input, pos_input, gtag_input, sylls_input, concepts_input], outputs = y_output)\n",
    "model.compile(optimizer = \"adam\", loss = loss, metrics = [crf.accuracy])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "1. Epochs: 5\n",
    "2. Batch size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 781020,
     "status": "error",
     "timestamp": 1594739616584,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "4c8VNDn4aOu0",
    "outputId": "40fc16e4-3b11-45e4-ebdc-cac8dd717899",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "checkpoint = ModelCheckpoint(sst_home + 'model_final/best_model.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "history = model.fit([X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_concepts_train], y_train,\n",
    "                    batch_size = 32,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzw-hiGjVD-2"
   },
   "outputs": [],
   "source": [
    "model.load_weights(sst_home + 'model_3/best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Predictions\n",
    "\n",
    "The following code generates the annotation files according to the predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(y_pred, offset, file_name, path_to_save):\n",
    "  i = 1\n",
    "  has_begin = False\n",
    "  text = ''\n",
    "  file = path_to_save + file_name + '.ann'\n",
    "  for pred_sent, offset_sent in zip(y_pred, offset):\n",
    "    #fixed_sent = fixSeqTagged(pred_sent, offset_sent)\n",
    "    for ann, info in zip(pred_sent, offset_sent):\n",
    "      if ann[0] == SINGLE:\n",
    "        if has_begin:\n",
    "          ##print('ERROR: Se ha comenzado una entidad sin terminar la anterior')\n",
    "          text = text + 'T{}\\tMORFOLOGIA_NEOPLASIA {} {}\\t{}\\n'.format(i, start, end, entity.replace('\\n', '#SALTO'))\n",
    "          i = i + 1\n",
    "          has_begin = False\n",
    "\n",
    "        text = text + 'T{}\\tMORFOLOGIA_NEOPLASIA {} {}\\t{}\\n'.format(i, info[1], info[2], info[0].replace('\\n', '#SALTO'))\n",
    "        i = i + 1\n",
    "\n",
    "      if ann[0] == BEGIN:\n",
    "        if has_begin:\n",
    "          ##print('ERROR: Se ha comenzado una entidad sin terminar la anterior')\n",
    "          text = text + 'T{}\\tMORFOLOGIA_NEOPLASIA {} {}\\t{}\\n'.format(i, start, end, entity.replace('\\n', '#SALTO'))\n",
    "          i = i + 1\n",
    "\n",
    "        entity = info[0]\n",
    "        start = info[1]\n",
    "        end = info[2]\n",
    "        has_begin = True\n",
    "\n",
    "      if ann[0] == INSIDE:\n",
    "        if not has_begin:\n",
    "          ##print('ERROR: Estamos en mitad de una entidad sin haber comenzado')\n",
    "          entity = info[0]\n",
    "          start = info[1]\n",
    "          end = info[2]\n",
    "          has_begin = True\n",
    "\n",
    "        else:\n",
    "          entity = entity + ' ' + info[0]\n",
    "          end = info[2]\n",
    "      \n",
    "      if ann[0] == END:\n",
    "        if not has_begin:\n",
    "          ##print('ERROR: Alcanzamos el final de una entidad sin tener principio')\n",
    "          text = text + 'T{}\\tMORFOLOGIA_NEOPLASIA {} {}\\t{}\\n'.format(i, info[1], info[2], info[0].replace('\\n', '#SALTO'))\n",
    "          i = i + 1\n",
    "        else:\n",
    "          entity_text = entity + ' ' + info[0]\n",
    "          text = text + 'T{}\\tMORFOLOGIA_NEOPLASIA {} {}\\t{}\\n'.format(i, start, info[2], entity_text.replace('\\n', '#SALTO'))\n",
    "          has_begin = False\n",
    "          i = i + 1\n",
    "\n",
    "  with open(file, 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 496611,
     "status": "ok",
     "timestamp": 1594726789665,
     "user": {
      "displayName": "SERGIO SANTAMARIA CARRASCO",
      "photoUrl": "",
      "userId": "08307977480150782938"
     },
     "user_tz": -120
    },
    "id": "NAcTCoGgPh8m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_concepts = pd.read_csv(sst_home + 'info_test.csv')\n",
    "df_concepts.drop_duplicates()\n",
    "df_concepts = df_concepts[df_concepts['CLASE'].isin(classes)]\n",
    "sst_home_test = sst_home + 'test-background-set-to-publish/'\n",
    "_, _docs, _docs_tags, offset = getElements(sst_home_test, MAX_SEQ_LEN, df_concepts, classes, getTags = False)\n",
    "idx2tag = {idx:tag for (tag, idx) in tag2idx.items()}\n",
    "\n",
    "result_path = sst_home + 'final-result/'\n",
    "for (file_name, doc_sents) in _docs.items():\n",
    "  X_char = getCharacterInput(_docs[file_name][0], MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "  X_words = getWordInput(_docs[file_name][0], MAX_SEQ_LEN, word2idx)\n",
    "  X_pos = getPosInput(_docs[file_name][1], MAX_SEQ_LEN, pos2idx)\n",
    "  X_tag = getGTagInput(_docs[file_name][2], MAX_SEQ_LEN, grammtags2idx)\n",
    "  X_syll = getSyllsInput(_docs[file_name][0], MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "  X_concepts = getConceptsInput(_docs[file_name][3], MAX_SEQ_LEN, classes)\n",
    "\n",
    "  #y_test = getY(_tags, MAX_SEQ_LEN, tag2idx)\n",
    "  y_pred = model.predict([X_char, X_words, X_pos, X_tag, X_syll, X_concepts])\n",
    "  y_pred = [list(map(lambda x: idx2tag[np.argmax(x)], sent)) for sent in y_pred]\n",
    "  offset_test = offset[file_name]\n",
    "  annotate(y_pred, offset_test, file_name, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = [_docs, _docs_tags, offset, idx2tag]\n",
    "with open(sst_home + 'data_final/prediction.pickle', 'wb') as handle:\n",
    "    pickle.dump(pred_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNdTOYOXOTbsLB4Dwz+GKPf",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
